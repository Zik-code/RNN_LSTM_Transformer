{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd5bb9c1",
   "metadata": {},
   "source": [
    "RNN_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "327d2d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN_model.py\n",
    "import torch\n",
    "from torch import nn\n",
    " \n",
    " \n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    input_size (int):输入数据的特征大小，即每个时间步的输入向量的维度。\n",
    "    hidden_size (int):隐藏层的特征大小，即每个时间步的隐藏状态向量的维度。\n",
    "    num_layers (int,可选):RNN的层数，默认值为1。当层数大于1时，RNN会变为多层RNN。\n",
    "    nonlinearity (str,可选):指定激活函数，默认值为'tanh'。可选值有'tanh'和'relu'。\n",
    "    bias (bool,可选):如果设置为True，则在RNN中添加偏置项。默认值为True。\n",
    "    batch_first (bool,可选):如果设置为True，则输入数据的形状为(batch_size, seq_len, input_size)。否则，默认输入数据的形状为(seq_len, batch_size, input_size)。默认值为False。\n",
    "    dropout (float,可选):如果非零，则在除最后一层之外的每个RNN层之间添加dropout层，其丢弃概率为dropout。默认值为0。\n",
    "    bidirectional (bool,可选):如果设置为True，则使用双向RNN。默认值为False。\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    " \n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim # 隐藏状态 ht 的维度\n",
    "        self.n_layers = n_layers # 网络的层数\n",
    " \n",
    "        # Defining the layers\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    " \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    " \n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    " \n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    " \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    " \n",
    "        return out, hidden\n",
    " \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398a49dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03c78b03",
   "metadata": {},
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5accaadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxlen= 15\n",
      "Input Sequence: hey how are yo\n",
      "Target Sequence: ey how are you\n",
      "Input Sequence: good i am fine\n",
      "Target Sequence: ood i am fine \n",
      "Input Sequence: have a nice da\n",
      "Target Sequence: ave a nice day\n",
      "Epoch: 10/100............. Loss: 2.4375\n",
      "Epoch: 20/100............. Loss: 2.2258\n",
      "Epoch: 30/100............. Loss: 1.9252\n",
      "Epoch: 40/100............. Loss: 1.5377\n",
      "Epoch: 50/100............. Loss: 1.1111\n",
      "Epoch: 60/100............. Loss: 0.7578\n",
      "Epoch: 70/100............. Loss: 0.5231\n",
      "Epoch: 80/100............. Loss: 0.3724\n",
      "Epoch: 90/100............. Loss: 0.2755\n",
      "Epoch: 100/100............. Loss: 0.2110\n"
     ]
    }
   ],
   "source": [
    " \n",
    "# train.py\n",
    "import torch\n",
    "from torch import nn\n",
    " \n",
    "import numpy as np\n",
    " \n",
    "# 首先，我们将定义我们希望模型在输入第一个单词或前几个字符时输出的句子。\n",
    "# 然后我们将从句子中的所有字符创建一个字典，并将它们映射到一个整数。\n",
    "# 这将允许我们将输入字符转换为它们各自的整数（char2int），反之亦然（int2char）。\n",
    " \n",
    "text = ['hey how are you', 'good i am fine', 'have a nice day']\n",
    " \n",
    "chars = set(''.join(text))\n",
    "# print(chars)# 输出 : {'y', 'o', ' ', 'd', 'f', 'n', 'm', 'i', 'w', 'r', 'u', 'v', 'h', 'c', 'g', 'e', 'a'} (注意:输出不定，但都包含了所有的字符)\n",
    "\n",
    "int2char = dict(enumerate(chars))\n",
    "# print('int2char=',int2char)\n",
    "# 输出：int2char= {0: 'a', 1: 'r', 2: 'u', ············}\n",
    " \n",
    "# 对 int2char反转\n",
    "char2int = {char: ind for ind, char in int2char.items()}\n",
    "# print(char2int)\n",
    "# 输出：char2int = {'e': 0, 'y': 1, 'm': 2, ···········}\n",
    " \n",
    "# ------------------------------------------------------------------------------------\n",
    "# 接下来，我们将填充(padding)输入句子以确保所有句子都是标准长度。\n",
    "# 虽然 RNN 通常能够接收可变大小的输入，但我们通常希望分批输入训练数据以加快训练过程。\n",
    "# 为了使用批次(batch)来训练我们的数据，我们需要确保输入数据中的每个序列大小相等。\n",
    " \n",
    "# 因此，在大多数情况下，可以通过用 0 值填充太短的序列和修剪太长的序列来完成填充。\n",
    "# 在我们的例子中，我们将找到最长序列的长度，并用空格填充其余句子以匹配该长度。\n",
    " \n",
    "# Finding the length of the longest string in our data\n",
    "maxlen = len(max(text, key=len))\n",
    "print('maxlen=',maxlen)\n",
    "# 输出 maxlen= 15，找到最长的句子hey how are 占15个字符。\n",
    "\n",
    "# 填充长度不足15的句子为15\n",
    "for i in range(len(text)):\n",
    "  while len(text[i])<maxlen:\n",
    "      text[i] += ' '\n",
    " \n",
    "# 由于我们要在每个时间步预测序列中的下一个字符，我们必须将每个句子分为：\n",
    " \n",
    "# 输入数据\n",
    "# 最后一个字符需排除因为它不需要作为模型的输入\n",
    "# 目标/真实标签\n",
    "# 它为每一个时刻后的值，因为这才是下一个时刻的值。\n",
    "# Creating lists that will hold our input and target sequences\n",
    "input_seq = []\n",
    "target_seq = []\n",
    " \n",
    "for i in range(len(text)):\n",
    "    # Remove last character for input sequence\n",
    "    input_seq.append(text[i][:-1])\n",
    " \n",
    "    # Remove first character for target sequence\n",
    "    target_seq.append(text[i][1:])\n",
    "    print(\"Input Sequence: {}\\nTarget Sequence: {}\".format(input_seq[i], target_seq[i]))\n",
    " \n",
    "# 现在我们可以通过使用上面创建的字典映射输入和目标序列到整数序列。\n",
    "# 这将允许我们随后对输入序列进行一次one-hot encoding。\n",
    " \n",
    "for i in range(len(text)):\n",
    "    input_seq[i] = [char2int[character] for character in input_seq[i]]\n",
    "    target_seq[i] = [char2int[character] for character in target_seq[i]]\n",
    " \n",
    "# 定义如下三个变量\n",
    "# dict_size: 字典的长度，即唯一字符的个数。它将决定one-hot vector的长度\n",
    "# seq_len:输入到模型中的sequence长度。这里是最长的句子的长度-1，因为不需要最后一个字符\n",
    "# batch_size: mini batch的大小，用于批量训练\n",
    "dict_size = len(char2int)\n",
    "seq_len = maxlen - 1\n",
    "batch_size = len(text)\n",
    " \n",
    " \n",
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
    " \n",
    "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            features[i, u, sequence[i][u]] = 1\n",
    "    return features\n",
    "# 同时定义一个helper function，用于初始化one-hot向量\n",
    "# Input shape --> (Batch Size, Sequence Length, One-Hot Encoding Size)\n",
    "input_seq = one_hot_encode(input_seq, dict_size, seq_len, batch_size)\n",
    " \n",
    "# 到此我们完成了所有的数据预处理，可以将数据从NumPy数组转为PyTorch张量啦\n",
    "input_seq = torch.from_numpy(input_seq)\n",
    "target_seq = torch.Tensor(target_seq)\n",
    " \n",
    "# 接下来就是搭建模型的步骤，你可以在这一步使用全连接层，卷积层，RNN层，LSTM层等等。\n",
    "# 但是我在这里使用最最基础的nn.rnn来示例一个RNN是如何使用的。\n",
    "from RNN_model import Model\n",
    " \n",
    "\"\"\"\n",
    "# 在开始构建模型之前，让我们使用 PyTorch 中的内置功能来检查我们正在运行的设备（CPU 或 GPU）。\n",
    "# 此实现不需要 GPU，因为训练非常简单。\n",
    "# 但是，随着处理具有数百万个可训练参数的大型数据集和模型，使用 GPU 对加速训练非常重要。\n",
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "# is_cuda = torch.cuda.is_available()\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "# if is_cuda:\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     print(\"GPU is available\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "#     print(\"GPU not available, CPU used\")\n",
    "\"\"\"\n",
    " \n",
    " \n",
    "# 要开始构建我们自己的神经网络模型，我们可以为所有神经网络模块定义一个继承 PyTorch 的基类（nn.module）的类。\n",
    "# 这样做之后，我们可以开始在构造函数下定义一些变量以及模型的层。 对于这个模型，我们将只使用一层 RNN，然后是一个全连接层。 全连接层将负责将 RNN 输出转换为我们想要的输出形状。\n",
    "# 我们还必须将 forward() 下的前向传递函数定义为类方法。 前向函数是按顺序执行的，因此我们必须先将输入和零初始化隐藏状态通过 RNN 层，然后再将 RNN 输出传递到全连接层。 请注意，我们使用的是在构造函数中定义的层。\n",
    "# 我们必须定义的最后一个方法是我们之前调用的用于初始化hidden state的方法 - init_hidden()。 这基本上会在我们的隐藏状态的形状中创建一个零张量。\n",
    " \n",
    " \n",
    " \n",
    "# 在定义了上面的模型之后，我们必须用相关参数实例化模型并定义我们的超参数。 我们在下面定义的超参数是：\n",
    "# n_epochs: 模型训练所有数据集的次数\n",
    "# lr: learning rate学习率\n",
    " \n",
    "# 与其他神经网络类似，我们也必须定义优化器和损失函数。 我们将使用 CrossEntropyLoss，因为最终输出基本上是一个分类任务和常见的 Adam 优化器。\n",
    "# Instantiate the model with hyperparameters\n",
    "model = Model(input_size=dict_size, output_size=dict_size, hidden_dim=12, n_layers=1)\n",
    "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
    "# model.to(device)\n",
    "# Define hyperparameters\n",
    "n_epochs = 100 # 训练轮数\n",
    "lr = 0.01 # 学习率\n",
    "# Define Loss, Optimizer\n",
    "loss_fn = nn.CrossEntropyLoss() # 交叉熵损失函数\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr) # 采用Adam作为优化器\n",
    "# 现在我们可以开始训练了！\n",
    "# 由于我们只有几句话，所以这个训练过程非常快。\n",
    "# 然而，随着我们的进步，更大的数据集和更深的模型意味着输入数据要大得多，并且我们必须计算的模型中的参数数量要多得多。\n",
    "# Training Run\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    optimizer.zero_grad()  # Clears existing gradients from previous epoch\n",
    "    # input_seq.to(device) # 使用GPU\n",
    "    output, hidden = model(input_seq)\n",
    "    loss = loss_fn(output, target_seq.view(-1).long())\n",
    "    loss.backward()  # Does backpropagation and calculates gradients\n",
    "    optimizer.step()  # Updates the weights accordingly\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "        print(\"Loss: {:.4f}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbdab92",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "821638f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxlen= 15\n",
      "Input Sequence: hey how are yo\n",
      "Target Sequence: ey how are you\n",
      "Input Sequence: good i am fine\n",
      "Target Sequence: ood i am fine \n",
      "Input Sequence: have a nice da\n",
      "Target Sequence: ave a nice day\n",
      "Epoch: 10/100............. Loss: 2.3955\n",
      "Epoch: 20/100............. Loss: 2.1573\n",
      "Epoch: 30/100............. Loss: 1.7791\n",
      "Epoch: 40/100............. Loss: 1.3466\n",
      "Epoch: 50/100............. Loss: 0.9660\n",
      "Epoch: 60/100............. Loss: 0.6816\n",
      "Epoch: 70/100............. Loss: 0.4873\n",
      "Epoch: 80/100............. Loss: 0.3568\n",
      "Epoch: 90/100............. Loss: 0.2667\n",
      "Epoch: 100/100............. Loss: 0.2046\n"
     ]
    }
   ],
   "source": [
    "# test.py\n",
    "# 现在让我们测试我们的模型，看看我们会得到什么样的输出。 作为第一步，我们将定义一些辅助函数来将我们的模型输出转换回文本。\n",
    "# This function takes in the model and character as arguments and returns the next character prediction and hidden state\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import device\n",
    "import torch.nn as nn\n",
    "from train import char2int, one_hot_encode, dict_size, int2char, model\n",
    " \n",
    " \n",
    "def predict(model, character):\n",
    "    # One-hot encoding our input to fit into the model\n",
    "    character = np.array([[char2int[c] for c in character]])\n",
    "    character = one_hot_encode(character, dict_size, character.shape[1], 1)\n",
    "    character = torch.from_numpy(character)\n",
    "    # character.to(device)\n",
    " \n",
    "    out, hidden = model(character)\n",
    " \n",
    "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
    " \n",
    "    # Taking the class with the highest probability score from the output\n",
    "    char_ind = torch.max(prob, dim=0)[1].item()\n",
    " \n",
    "    return int2char[char_ind], hidden\n",
    " \n",
    "# This function takes the desired output length and input characters as arguments, returning the produced sentence\n",
    "def sample(model, out_len, start='hey'):\n",
    "    model.eval() # eval mode\n",
    "    start = start.lower()\n",
    "    # First off, run through the starting characters\n",
    "    chars = [ch for ch in start]\n",
    "    size = out_len - len(chars)\n",
    "    # Now pass in the previous characters and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(model, chars)\n",
    "        chars.append(char)\n",
    " \n",
    "    return ''.join(chars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anomaly_Detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
