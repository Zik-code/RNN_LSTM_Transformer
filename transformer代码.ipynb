{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_seq_len=80):\n",
    "        super().__init__()\n",
    "        # d_model 表示输入嵌入向量的维度，在 Transformer 模型中，通常是一个固定的值\n",
    "        self.d_model = d_model\n",
    "        # pe为待生成的位置编码矩阵，创建一个形状为 (max_seq_len, d_model) 的全零张量\n",
    "        # max_seq_len 输入序列的最大长度，即句子的最大单词数。\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        # 遍历每个位置\n",
    "        for pos in range(max_seq_len):\n",
    "            # 遍历 d_model 中的偶数索引，因为位置编码在偶数和奇数位置上有不同的计算方式\n",
    "            for i in range(0, d_model, 2):\n",
    "                # 计算偶数位置的位置编码，使用正弦函数\n",
    "                pe[pos, i] = math.sin(pos / (10000**((2 * i) / d_model)))\n",
    "                # 计算奇数位置的位置编码，使用余弦函数\n",
    "                # 注意这里要确保 i + 1 不超过 d_model 的范围\n",
    "                if i + 1 < d_model:\n",
    "                    pe[pos, i + 1] = math.cos(pos / (10000**((2 * (i + 1)) / d_model)))\n",
    "        # 在第 0 维上增加一个维度，将 pe 的形状从 (max_seq_len, d_model) 变为 (1, max_seq_len, d_model)\n",
    "        # 这是为了方便后续与输入的批量数据进行广播操作\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # 将 pe 注册为缓冲区，这样它会成为模型状态的一部分，但不会被视为模型的参数（即不会被优化器更新）\n",
    "        # 这样做的好处是在保存和加载模型时，位置编码信息也会被保存和加载\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 在将输入的嵌入向量与位置编码相加之前，先将输入向量乘以 sqrt(d_model)\n",
    "        # 这是为了使得输入的嵌入向量的数值相对大一些，避免位置编码的影响过小\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        # 获取输入序列的长度\n",
    "        seq_len = x.size(1)\n",
    "        # 将输入的嵌入向量与位置编码相加\n",
    "        # 由于 pe 的形状是 (1, max_seq_len, d_model)，我们只取前 seq_len 个位置的编码信息\n",
    "        # 这样可以确保位置编码的长度与输入序列的长度一致\n",
    "        x = x + self.pe[:, :seq_len]\n",
    "        # 返回添加了位置编码的输入向量\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_seq_len=80):\n",
    "        super().__init__()\n",
    "        # d_model 表示输入嵌入向量的维度，在 Transformer 模型中，通常是一个固定的值\n",
    "        self.d_model = d_model\n",
    "        # pe为待生成的位置编码矩阵，创建一个形状为 (max_seq_len, d_model) 的全零张量\n",
    "        # max_seq_len 输入序列的最大长度，即句子的最大单词数。\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        # 遍历每个位置\n",
    "        for pos in range(max_seq_len):\n",
    "            # 遍历 d_model 中的偶数索引，因为位置编码在偶数和奇数位置上有不同的计算方式\n",
    "            for i in range(0, d_model, 2):\n",
    "                # 计算偶数位置的位置编码，使用正弦函数\n",
    "                pe[pos, i] = math.sin(pos / (10000**((2 * i) / d_model)))\n",
    "                # 计算奇数位置的位置编码，使用余弦函数\n",
    "                # 注意这里要确保 i + 1 不超过 d_model 的范围\n",
    "                if i + 1 < d_model:\n",
    "                    pe[pos, i + 1] = math.cos(pos / (10000**((2 * (i + 1)) / d_model)))\n",
    "        # 在第 0 维上增加一个维度，将 pe 的形状从 (max_seq_len, d_model) 变为 (1, max_seq_len, d_model)\n",
    "        # 这是为了方便后续与输入的批量数据进行广播操作\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # 将 pe 注册为缓冲区，这样它会成为模型状态的一部分，但不会被视为模型的参数（即不会被优化器更新）\n",
    "        # 这样做的好处是在保存和加载模型时，位置编码信息也会被保存和加载\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 在将输入的嵌入向量与位置编码相加之前，先将输入向量乘以 sqrt(d_model)\n",
    "        # 这是为了使得输入的嵌入向量的数值相对大一些，避免位置编码的影响过小\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        # 获取输入序列的长度\n",
    "        seq_len = x.size(1)\n",
    "        # 将输入的嵌入向量与位置编码相加\n",
    "        # 由于 pe 的形状是 (1, max_seq_len, d_model)，我们只取前 seq_len 个位置的编码信息\n",
    "        # 这样可以确保位置编码的长度与输入序列的长度一致\n",
    "        x = x + self.pe[:, :seq_len]\n",
    "        # 返回添加了位置编码的输入向量\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多头注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "heads：它表示注意力头的数量。多头注意力机制通过将输入的特征向量分割成多个头，\n",
    "每个头独立地计算注意力，从而可以捕捉到不同方面的信息。\n",
    "\n",
    "例如，设置 heads = 8 表示将输入的特征向量分割成 8 个子空间进行处理。\n",
    "\n",
    "### d_k 的计算和作用\n",
    "计算方式：self.d_k = d_model // heads 这行代码使用整数除法\n",
    "\n",
    "每个注意力头的维度 是d_k。\n",
    "\n",
    "例如，如果 d_model = 512，heads = 8，那么 d_k = 512 // 8 = 64。\n",
    "\n",
    "将多头注意力机制和掩码注意力机制写在一个类中\n",
    "判断是否有掩码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# 定义多头注意力机制类，继承自 PyTorch 的 nn.Module 类\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, heads, d_model, dropout=0.1):\n",
    "        \"\"\"\n",
    "        初始化多头注意力机制。\n",
    "\n",
    "        参数:\n",
    "        heads (int): 注意力头的数量。\n",
    "        d_model (int): 输入嵌入向量的维度。\n",
    "        dropout (float): 丢弃率，用于防止过拟合，默认值为 0.1。\n",
    "        \"\"\"\n",
    "        # 调用父类 nn.Module 的构造函数\n",
    "        super().__init__()\n",
    "        # 保存输入嵌入向量的维度\n",
    "        self.d_model = d_model\n",
    "       \n",
    "        # 保存注意力头的数量\n",
    "        self.h = heads\n",
    "        # 定义Q,K,V的线性变换层，将输入的 d_model 维度映射到 d_model 维度\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        # 定义丢弃层，用于防止过拟合\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 定义输出的线性变换层，将多头注意力的输出映射回 d_model 维度\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def attention(self, q, k, v, d_k, mask=None, dropout=None):\n",
    "        \"\"\"\n",
    "        计算注意力分数并应用注意力机制。\n",
    "\n",
    "        参数:\n",
    "        q (torch.Tensor): 查询张量，形状为 (batch_size, heads, seq_len, d_k)。\n",
    "        k (torch.Tensor): 键张量，形状为 (batch_size, heads, seq_len, d_k)。\n",
    "        v (torch.Tensor): 值张量，形状为 (batch_size, heads, seq_len, d_k)。\n",
    "        d_k (int): 每个注意力头的维度。\n",
    "        mask (torch.Tensor, 可选): 掩码张量，用于屏蔽某些位置的注意力分数，默认为 None。\n",
    "        dropout (nn.Dropout, 可选): 丢弃层，用于防止过拟合，默认为 None。\n",
    "\n",
    "        返回:\n",
    "        torch.Tensor: 注意力机制的输出，形状为 (batch_size, heads, seq_len, d_k)。\n",
    "        \"\"\"\n",
    "        # 计算查询和键的点积，得到注意力分数\n",
    "        # k.transpose(-2, -1) 将 k 的最后两个维度交换，以便进行矩阵乘法\n",
    "        # 除以 math.sqrt(d_k) 是为了缩放注意力分数，防止梯度消失或爆炸\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        # 如果提供了掩码，这里的掩码是与------相同形状的矩阵。\n",
    "        if mask is not None:\n",
    "            # 在第 1 维上增加一个维度，以便与 scores 张量进行广播操作\n",
    "            mask = mask.unsqueeze(1)\n",
    "            # 使用掩码将需要屏蔽的位置的注意力分数设置为负无穷大\n",
    "            # 这样在后续的 softmax 操作中，这些位置的概率将趋近于 0\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        # 对注意力分数应用 softmax 函数，得到注意力分布\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        # 如果提供了丢弃层\n",
    "        if dropout is not None:\n",
    "            # 对注意力分布应用丢弃操作，防止过拟合\n",
    "            scores = dropout(scores)\n",
    "        # 将注意力分布与值张量相乘，得到注意力机制的输出\n",
    "        output = torch.matmul(scores, v)\n",
    "        return output\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        前向传播函数，实现多头注意力机制的计算。\n",
    "\n",
    "        参数:\n",
    "        q (torch.Tensor): 查询输入，形状为 (batch_size, seq_len, d_model)。\n",
    "        k (torch.Tensor): 键输入，形状为 (batch_size, seq_len, d_model)。\n",
    "        v (torch.Tensor): 值输入，形状为 (batch_size, seq_len, d_model)。\n",
    "        mask (torch.Tensor, 可选): 掩码张量，用于屏蔽某些位置的注意力分数，默认为 None。\n",
    "\n",
    "        返回:\n",
    "        torch.Tensor: 多头注意力机制的输出，形状为 (batch_size, seq_len, d_model)。\n",
    "        \"\"\"\n",
    "        # 获取批量大小\n",
    "        bs = q.size(0)\n",
    "        # 对键输入进行线性变换，然后将其分割成多个头\n",
    "        # view(bs, -1, self.h, self.d_k) 将张量重塑为 (batch_size, seq_len, heads, d_k)\n",
    "        # transpose(1, 2) 将第 1 维和第 2 维交换，得到 (batch_size, heads, seq_len, d_k)\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        # 对查询输入进行线性变换，然后将其分割成多个头\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        # 对值输入进行线性变换，然后将其分割成多个头\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        # 调用 attention 函数计算多头注意力分数\n",
    "        scores = self.attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        # 将多头注意力的输出进行合并\n",
    "        # transpose(1, 2) 将第 1 维和第 2 维交换，得到 (batch_size, seq_len, heads, d_k)\n",
    "        # contiguous() 确保张量在内存中是连续的，以便后续的 view 操作能够正常进行\n",
    "        # view(bs, -1, self.d_model) 将张量重塑为 (batch_size, seq_len, d_model)\n",
    "        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n",
    "        # 对合并后的输出进行线性变换，得到最终的多头注意力机制的输出\n",
    "        output = self.out(concat)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义前馈神经网络类，继承自 PyTorch 的 nn.Module 类\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
    "        \"\"\"\n",
    "        初始化前馈神经网络层。\n",
    "\n",
    "        参数:\n",
    "        d_model (int): 输入和输出的特征维度，通常是模型的嵌入维度。\n",
    "        d_ff (int): 前馈网络隐藏层的维度，默认值为 2048。\n",
    "        dropout (float): 丢弃率，用于防止过拟合，默认值为 0.1。\n",
    "        \"\"\"\n",
    "        # 调用父类 nn.Module 的构造函数\n",
    "        super().__init__()\n",
    "        # 定义第一个线性变换层，将输入的 d_model 维度映射到 d_ff 维度\n",
    "        # 这里的 d_ff 是前馈网络隐藏层的维度，默认设置为 2048\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        # 定义丢弃层，用于防止过拟合\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 定义第二个线性变换层，将 d_ff 维度映射回 d_model 维度\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播函数，实现前馈神经网络的计算。\n",
    "\n",
    "        参数:\n",
    "        x (torch.Tensor): 输入张量，形状通常为 (batch_size, seq_len, d_model)。\n",
    "\n",
    "        返回:\n",
    "        torch.Tensor: 前馈神经网络的输出，形状为 (batch_size, seq_len, d_model)。\n",
    "        \"\"\"\n",
    "        # 首先将输入 x 通过第一个线性变换层\n",
    "        # 然后使用 ReLU 激活函数引入非线性\n",
    "        # 最后应用丢弃层防止过拟合\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        # 将经过非线性变换和丢弃操作后的结果通过第二个线性变换层\n",
    "        # 得到最终的输出，其维度与输入维度相同，都是 d_model\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义层归一化层类，继承自 PyTorch 的 nn.Module 类\n",
    "class NormLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        \"\"\"\n",
    "        初始化层归一化层。\n",
    "\n",
    "        参数:\n",
    "        d_model (int): 输入张量的特征维度，即每个样本的特征数量。\n",
    "        eps (float): 一个小的常量，用于数值稳定性，防止分母为零，默认值为 1e-6。\n",
    "        \"\"\"\n",
    "        # 调用父类 nn.Module 的构造函数\n",
    "        super().__init__()\n",
    "        # 保存输入张量的特征维度\n",
    "        self.size = d_model\n",
    "        # 层归一化包含两个可学习的参数\n",
    "        # alpha 是缩放因子，初始化为全 1 的张量，形状为 (d_model,)\n",
    "        # 使用 nn.Parameter 将其包装为可训练的参数，在训练过程中会自动更新\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        # bias 是偏移量，初始化为全 0 的张量，形状为 (d_model,)\n",
    "        # 同样使用 nn.Parameter 将其包装为可训练的参数\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        # 保存用于数值稳定性的小常量\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播函数，实现层归一化的计算。\n",
    " \n",
    "        参数:\n",
    "        x (torch.Tensor): 输入张量，形状通常为 (batch_size, seq_len, d_model)。\n",
    "\n",
    "        返回:\n",
    "        torch.Tensor: 层归一化后的输出张量，形状与输入相同，为 (batch_size, seq_len, d_model)。\n",
    "        \"\"\"\n",
    "        # 计算输入张量在最后一个维度（特征维度）上的均值\n",
    "        # dim=-1 表示在最后一个维度上计算，keepdim=True 表示保持维度不变\n",
    "        # 这样得到的均值张量形状与输入张量相同，方便后续计算\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        # 计算输入张量在最后一个维度（特征维度）上的标准差\n",
    "        # 同样在最后一个维度上计算，保持维度不变\n",
    "        # 加上 self.eps 是为了防止标准差为零，保证数值稳定性\n",
    "        std = x.std(dim=-1, keepdim=True) + self.eps\n",
    "        # 进行层归一化操作\n",
    "        # 先将输入张量减去均值，再除以标准差，得到归一化后的张量\n",
    "        # 然后乘以可学习的缩放因子 alpha，再加上可学习的偏移量 bias\n",
    "        norm = self.alpha * ((x - mean) / std) + self.bias\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 假设 PositionalEncoder、EncoderLayer 和 NormLayer 类已经定义\n",
    "# 这里只是使用它们，具体定义可参考之前的代码\n",
    "\n",
    "# 定义编码器类，继承自 PyTorch 的 nn.Module 类\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
    "        \"\"\"\n",
    "        初始化编码器。\n",
    "\n",
    "        参数:\n",
    "        vocab_size (int): 词汇表的大小，即不同单词的数量。\n",
    "        d_model (int): 模型的嵌入维度，也就是每个单词的嵌入向量的维度。\n",
    "        N (int): 编码器层的数量，即堆叠的 EncoderLayer 的数量。\n",
    "        heads (int): 多头注意力机制中注意力头的数量。\n",
    "        dropout (float): 丢弃率，用于防止过拟合。\n",
    "        \"\"\"\n",
    "        # 调用父类 nn.Module 的构造函数\n",
    "        super().__init__()\n",
    "        # 保存编码器层的数量\n",
    "        self.N = N\n",
    "        # 定义词嵌入层，将输入的单词索引转换为对应的嵌入向量\n",
    "        # vocab_size 是词汇表的大小，d_model 是嵌入向量的维度\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        # 创建位置编码器实例，用于为输入的嵌入向量添加位置信息\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        # 创建一个 ModuleList，其中包含 N 个 EncoderLayer 实例\n",
    "        # 每个 EncoderLayer 包含多头注意力机制和前馈神经网络\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, heads, dropout) for _ in range(N)])\n",
    "        # 创建层归一化层实例，用于对编码器的输出进行归一化处理\n",
    "        self.norm = NormLayer(d_model)\n",
    "\n",
    "    def forward(self, src, mask):\n",
    "        \"\"\"\n",
    "        前向传播函数，实现编码器的计算。\n",
    "\n",
    "        参数:\n",
    "        src (torch.Tensor): 输入的源序列，通常是单词的索引序列，形状为 (batch_size, seq_len)。\n",
    "        mask (torch.Tensor): 掩码张量，用于屏蔽某些位置的注意力分数，形状根据具体需求而定。\n",
    "\n",
    "        返回:\n",
    "        torch.Tensor: 编码器的输出，形状为 (batch_size, seq_len, d_model)。\n",
    "        \"\"\"\n",
    "        # 通过词嵌入层将输入的单词索引转换为嵌入向量\n",
    "        # 输出的形状为 (batch_size, seq_len, d_model)\n",
    "        x = self.embed(src)\n",
    "        # 为嵌入向量添加位置编码信息\n",
    "        # 位置编码可以让模型感知到单词在序列中的位置\n",
    "        x = self.pe(x)\n",
    "        # 依次通过 N 个 EncoderLayer 进行处理\n",
    "        for layer in self.layers:\n",
    "            # 每个 EncoderLayer 都会对输入进行多头注意力和前馈神经网络的计算\n",
    "            x = layer(x, mask)\n",
    "        # 最后对编码器的输出进行层归一化处理\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 假设 MultiHeadAttention、FeedForward 和 NormLayer 类已经定义\n",
    "# 这里直接使用它们\n",
    "\n",
    "# 定义编码器层类，继承自 PyTorch 的 nn.Module 类\n",
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        \"\"\"\n",
    "        初始化编码器层。\n",
    "\n",
    "        参数:\n",
    "        d_model (int): 模型的嵌入维度，即输入和输出的特征维度。\n",
    "        heads (int): 多头注意力机制中注意力头的数量。\n",
    "        dropout (float): 丢弃率，用于防止过拟合，默认值为 0.1。\n",
    "        \"\"\"\n",
    "        # 调用父类 nn.Module 的构造函数\n",
    "        super().__init__()\n",
    "        # 第一个层归一化层，用于在多头注意力机制前对输入进行归一化\n",
    "        self.norm_1 = NormLayer(d_model)\n",
    "        # 第二个层归一化层，用于在前馈神经网络前对输入进行归一化\n",
    "        self.norm_2 = NormLayer(d_model)\n",
    "        # 多头注意力机制模块\n",
    "        self.attn = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
    "        # 前馈神经网络模块\n",
    "        self.ff = FeedForward(d_model, dropout=dropout)\n",
    "        # 第一个丢弃层，用于在多头注意力机制的输出上应用丢弃操作\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        # 第二个丢弃层，用于在前馈神经网络的输出上应用丢弃操作\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        前向传播函数，实现编码器层的计算。\n",
    "\n",
    "        参数:\n",
    "        x (torch.Tensor): 输入张量，形状通常为 (batch_size, seq_len, d_model)。\n",
    "        mask (torch.Tensor): 掩码张量，用于屏蔽某些位置的注意力分数，形状根据具体需求而定。\n",
    "\n",
    "        返回:\n",
    "        torch.Tensor: 编码器层的输出，形状与输入相同，为 (batch_size, seq_len, d_model)。\n",
    "        \"\"\"\n",
    "        # 首先对输入进行层归一化\n",
    "        x2 = self.norm_1(x)\n",
    "        # 将归一化后的输入传入多头注意力机制进行计算\n",
    "        # 这里查询（Query）、键（Key）和值（Value）都使用归一化后的输入 x2\n",
    "        attn_output = self.attn(x2, x2, x2, mask)\n",
    "        # 对多头注意力机制的输出应用丢弃操作，防止过拟合\n",
    "        attn_output = self.dropout_1(attn_output)\n",
    "        # 将多头注意力机制的输出与原始输入进行残差连接\n",
    "        # 残差连接有助于缓解梯度消失问题，使模型更容易训练\n",
    "        x = x + attn_output\n",
    "        # 对经过多头注意力机制和残差连接后的输出再次进行层归一化\n",
    "        x2 = self.norm_2(x)\n",
    "        # 将归一化后的输出传入前馈神经网络进行计算\n",
    "        ff_output = self.ff(x2)\n",
    "        # 对前馈神经网络的输出应用丢弃操作，防止过拟合\n",
    "        ff_output = self.dropout_2(ff_output)\n",
    "        # 将前馈神经网络的输出与经过多头注意力机制处理后的输入进行残差连接\n",
    "        x = x + ff_output\n",
    "        # 返回最终的输出\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 假设 PositionalEncoder、DecoderLayer 和 NormLayer 类已经定义，这里直接使用\n",
    "\n",
    "# 定义解码器类，继承自 PyTorch 的 nn.Module 类\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
    "        \"\"\"\n",
    "        初始化解码器。\n",
    "\n",
    "        参数:\n",
    "        vocab_size (int): 目标词汇表的大小，即不同目标单词的数量。\n",
    "        d_model (int): 模型的嵌入维度，也就是每个单词的嵌入向量的维度。\n",
    "        N (int): 解码器层的数量，即堆叠的 DecoderLayer 的数量。\n",
    "        heads (int): 多头注意力机制中注意力头的数量。\n",
    "        dropout (float): 丢弃率，用于防止过拟合。\n",
    "        \"\"\"\n",
    "        # 调用父类的构造函数\n",
    "        super().__init__()\n",
    "        # 保存解码器层的数量\n",
    "        self.N = N\n",
    "        # 定义词嵌入层，将目标序列的单词索引转换为对应的嵌入向量\n",
    "        # vocab_size 是目标词汇表的大小，d_model 是嵌入向量的维度\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        # 创建位置编码器实例，用于为目标序列的嵌入向量添加位置信息\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        # 创建一个 ModuleList，其中包含 N 个 DecoderLayer 实例\n",
    "        # 每个 DecoderLayer 包含自注意力机制、编码器 - 解码器注意力机制和前馈神经网络\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, heads, dropout) for _ in range(N)])\n",
    "        # 创建层归一化层实例，用于对解码器的输出进行归一化处理\n",
    "        self.norm = NormLayer(d_model)\n",
    "\n",
    "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
    "        \"\"\"\n",
    "        前向传播函数，实现解码器的计算。\n",
    "\n",
    "        参数:\n",
    "        trg (torch.Tensor): 输入的目标序列，通常是目标单词的索引序列，形状为 (batch_size, trg_seq_len)。\n",
    "        e_outputs (torch.Tensor): 编码器的输出，形状为 (batch_size, src_seq_len, d_model)。\n",
    "        src_mask (torch.Tensor): 源序列的掩码张量，用于屏蔽源序列中的填充位置等，形状根据源序列而定。\n",
    "        trg_mask (torch.Tensor): 目标序列的掩码张量，用于屏蔽目标序列中的未来位置和填充位置等，形状根据目标序列而定。\n",
    "\n",
    "        返回:\n",
    "        torch.Tensor: 解码器的输出，形状为 (batch_size, trg_seq_len, d_model)。\n",
    "        \"\"\"\n",
    "        # 通过词嵌入层将输入的目标序列单词索引转换为嵌入向量\n",
    "        # 输出的形状为 (batch_size, trg_seq_len, d_model)\n",
    "        x = self.embed(trg)\n",
    "        # 为目标序列的嵌入向量添加位置编码信息\n",
    "        # 位置编码可以让模型感知到目标单词在序列中的位置\n",
    "        x = self.pe(x)\n",
    "        # 依次通过 N 个 DecoderLayer 进行处理\n",
    "        for layer in self.layers:\n",
    "            # 每个 DecoderLayer 都会对输入进行自注意力、编码器 - 解码器注意力和前馈神经网络的计算\n",
    "            x = layer(x, e_outputs, src_mask, trg_mask)\n",
    "        # 最后对解码器的输出进行层归一化处理\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 假设 MultiHeadAttention、FeedForward 和 NormLayer 类已经定义\n",
    "# 这里直接使用这些类\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        \"\"\"\n",
    "        初始化解码器层。\n",
    "\n",
    "        参数:\n",
    "        d_model (int): 模型的嵌入维度，即输入和输出张量的特征维度。\n",
    "        heads (int): 多头注意力机制中注意力头的数量。\n",
    "        dropout (float): 丢弃率，用于防止过拟合，默认值为 0.1。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 定义三个层归一化模块，分别用于不同子层的输入归一化\n",
    "        # 第一个层归一化用于自注意力机制的输入\n",
    "        self.norm_1 = NormLayer(d_model)\n",
    "        # 第二个层归一化用于编码器 - 解码器注意力机制的输入\n",
    "        self.norm_2 = NormLayer(d_model)\n",
    "        # 第三个层归一化用于前馈神经网络的输入\n",
    "        self.norm_3 = NormLayer(d_model)\n",
    "\n",
    "        # 定义三个丢弃层，分别应用于不同子层的输出，防止过拟合\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "\n",
    "        # 定义两个多头注意力模块\n",
    "        # 第一个多头注意力模块是自注意力机制，用于处理目标序列内部的依赖关系\n",
    "        self.attn_1 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
    "        # 第二个多头注意力模块是编码器 - 解码器注意力机制，用于结合编码器输出和目标序列信息\n",
    "        self.attn_2 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
    "\n",
    "        # 定义前馈神经网络模块，用于对特征进行进一步的非线性变换\n",
    "        self.ff = FeedForward(d_model, dropout=dropout)\n",
    "\n",
    "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
    "        \"\"\"\n",
    "        前向传播函数，实现解码器层的计算。\n",
    "\n",
    "        参数:\n",
    "        x (torch.Tensor): 输入的目标序列张量，形状为 (batch_size, trg_seq_len, d_model)。\n",
    "        e_outputs (torch.Tensor): 编码器的输出张量，形状为 (batch_size, src_seq_len, d_model)。\n",
    "        src_mask (torch.Tensor): 源序列的掩码张量，用于屏蔽源序列中的填充位置等，形状根据源序列而定。\n",
    "        trg_mask (torch.Tensor): 目标序列的掩码张量，用于屏蔽目标序列中的未来位置和填充位置等，形状根据目标序列而定。\n",
    "\n",
    "        返回:\n",
    "        torch.Tensor: 解码器层的输出张量，形状为 (batch_size, trg_seq_len, d_model)。\n",
    "        \"\"\"\n",
    "        # 1. 自注意力子层\n",
    "        # 对输入的目标序列进行层归一化\n",
    "        x2 = self.norm_1(x)\n",
    "        # 计算自注意力机制的输出，这里查询（Query）、键（Key）和值（Value）都使用归一化后的目标序列\n",
    "        # trg_mask 用于屏蔽目标序列中的未来位置和填充位置，防止模型看到未来信息\n",
    "        attn_1_output = self.attn_1(x2, x2, x2, trg_mask)\n",
    "        # 对自注意力机制的输出应用丢弃操作，防止过拟合\n",
    "        attn_1_output = self.dropout_1(attn_1_output)\n",
    "        # 进行残差连接，将自注意力机制的输出与原始输入相加\n",
    "        # 残差连接有助于缓解梯度消失问题，使模型更容易训练\n",
    "        x = x + attn_1_output\n",
    "\n",
    "        # 2. 编码器 - 解码器注意力子层\n",
    "        # 对经过自注意力子层处理后的目标序列再次进行层归一化\n",
    "        x2 = self.norm_2(x)\n",
    "        # 计算编码器 - 解码器注意力机制的输出\n",
    "        # 查询（Query）使用归一化后的目标序列，键（Key）和值（Value）使用编码器的输出\n",
    "        # src_mask 用于屏蔽源序列中的填充位置\n",
    "        attn_2_output = self.attn_2(x2, e_outputs, e_outputs, src_mask)\n",
    "        # 对编码器 - 解码器注意力机制的输出应用丢弃操作，防止过拟合\n",
    "        attn_2_output = self.dropout_2(attn_2_output)\n",
    "        # 进行残差连接，将编码器 - 解码器注意力机制的输出与经过自注意力子层处理后的输入相加\n",
    "        x = x + attn_2_output\n",
    "\n",
    "        # 3. 前馈神经网络子层\n",
    "        # 对经过编码器 - 解码器注意力子层处理后的目标序列进行层归一化\n",
    "        x2 = self.norm_3(x)\n",
    "        # 计算前馈神经网络的输出\n",
    "        ff_output = self.ff(x2)\n",
    "        # 对前馈神经网络的输出应用丢弃操作，防止过拟合\n",
    "        ff_output = self.dropout_3(ff_output)\n",
    "        # 进行残差连接，将前馈神经网络的输出与经过编码器 - 解码器注意力子层处理后的输入相加\n",
    "        x = x + ff_output\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# ========================================\n",
    "# 位置编码模块\n",
    "# ========================================\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len=80):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i) / d_model)))\n",
    "                if i + 1 < d_model:\n",
    "                    pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1)) / d_model)))\n",
    "        pe = pe.unsqueeze(0)  # 增加batch维度\n",
    "        self.register_buffer('pe', pe)  # 注册为非训练参数\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * math.sqrt(self.d_model)  # 缩放嵌入向量\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len]  # 广播机制自动匹配维度\n",
    "        return x\n",
    "\n",
    "# ========================================\n",
    "# 多头注意力模块\n",
    "# ========================================\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def attention(self, q, k, v, d_k, mask=None, dropout=None):\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        if dropout is not None:\n",
    "            scores = dropout(scores)\n",
    "        output = torch.matmul(scores, v)\n",
    "        return output\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        bs = q.size(0)\n",
    "        # 线性变换并分割多头\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        # 计算注意力\n",
    "        scores = self.attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        # 合并多头\n",
    "        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n",
    "        output = self.out(concat)\n",
    "        return output\n",
    "\n",
    "# ========================================\n",
    "# 前馈神经网络模块\n",
    "# ========================================\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "\n",
    "# ========================================\n",
    "# 层归一化模块\n",
    "# ========================================\n",
    "class NormLayer(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.size = d_model\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True) + self.eps\n",
    "        return self.alpha * (x - mean) / std + self.bias\n",
    "\n",
    "# ========================================\n",
    "# 编码器层模块\n",
    "# ========================================\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = NormLayer(d_model)\n",
    "        self.norm_2 = NormLayer(d_model)\n",
    "        self.attn = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
    "        self.ff = FeedForward(d_model, dropout=dropout)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = x + self.dropout_1(self.attn(self.norm_1(x), self.norm_1(x), self.norm_1(x), mask))\n",
    "        x = x + self.dropout_2(self.ff(self.norm_2(x)))\n",
    "        return x\n",
    "\n",
    "# ========================================\n",
    "# 编码器模块\n",
    "# ========================================\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, heads, dropout) for _ in range(N)])\n",
    "        self.norm = NormLayer(d_model)\n",
    "\n",
    "    def forward(self, src, mask):\n",
    "        x = self.pe(self.embed(src))\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "# ========================================\n",
    "# 解码器层模块\n",
    "# ========================================\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = NormLayer(d_model)\n",
    "        self.norm_2 = NormLayer(d_model)\n",
    "        self.norm_3 = NormLayer(d_model)\n",
    "        self.attn_1 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
    "        self.attn_2 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
    "        self.ff = FeedForward(d_model, dropout=dropout)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
    "        x = x + self.dropout_1(self.attn_1(self.norm_1(x), self.norm_1(x), self.norm_1(x), trg_mask))\n",
    "        x = x + self.dropout_2(self.attn_2(self.norm_2(x), e_outputs, e_outputs, src_mask))\n",
    "        x = x + self.dropout_3(self.ff(self.norm_3(x)))\n",
    "        return x\n",
    "\n",
    "# ========================================\n",
    "# 解码器模块\n",
    "# ========================================\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, heads, dropout) for _ in range(N)])\n",
    "        self.norm = NormLayer(d_model)\n",
    "\n",
    "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
    "        x = self.pe(self.embed(trg))\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, e_outputs, src_mask, trg_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "# ========================================\n",
    "# 完整Transformer模型\n",
    "# ========================================\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab, d_model, N, heads, dropout)\n",
    "        self.decoder = Decoder(trg_vocab, d_model, N, heads, dropout)\n",
    "        self.out = nn.Linear(d_model, trg_vocab)\n",
    "\n",
    "    def forward(self, src, trg, src_mask, trg_mask):\n",
    "        e_outputs = self.encoder(src, src_mask)\n",
    "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
    "        output = self.out(d_output)\n",
    "        try:\n",
    "            # 为了避免在生成时进行softmax，这里分开处理\n",
    "            return output\n",
    "        except:\n",
    "            return F.log_softmax(output, dim=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anomaly_Detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
