{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🌐 整体结构：流水线协作的翻译团队\n",
    "Transformer像一个高效的翻译流水线，分为**编码器（Encoder）**和**解码器（Decoder）**两部分：\n",
    "- **编码器**：处理原始语言（比如中文），生成「理解向量」\n",
    "- **解码器**：根据编码器的理解，生成目标语言（比如英文）\n",
    "\n",
    "每个编码器和解码器内部都有多个「专家模块」（类似团队中的不同角色）。\n",
    "\n",
    "\n",
    "### 🔍 核心机制：自注意力（Self-Attention）\n",
    "- 在自己所处的环境中，要注意别人的动向。\n",
    "#### 比喻：会议讨论时的注意力分配\n",
    "当翻译团队处理句子「猫追老鼠」时：\n",
    "1. **分解任务**：每个词（猫、追、老鼠）都生成三个角色，即每个字都被映射为三个向量：\n",
    "   \n",
    "   - **查询（Query）**：我需要什么信息？\n",
    "   - **键（Key）**：我能提供什么信息？\n",
    "   - **值（Value）**：我实际包含的信息\n",
    "\n",
    "2. **计算关联度**：\n",
    "   - 每个词的Query会和其他词的Key计算「注意力分数」，通常使用点积运算（越相似点积结果越大）：\\(scores_{i,j} = q_i^T k_j\\)，为了避免点积结果过大，在计算得分时通常会除以 \\(\\sqrt{d_k}\\)：\\(scores_{i,j} = \\frac{q_i^T k_j}{\\sqrt{d_k}}\\)\n",
    "   - 应用 Softmax 函数：\n",
    "将得分通过 Softmax 函数转换为概率分布，得到***注意力权重***：\n",
    "\\(attention_{i,j} = \\frac{\\exp(scores_{i,j})}{\\sum_{l = 1}^{n} \\exp(scores_{i,l})}\\)\n",
    "   - 计算加权和：\n",
    "根据注意力权重对 Value 进行加权求和，得到每个位置的输出，即生成当前词的最终表示（比如：「追」的最终表示会包含「猫」的主动性和「老鼠」的移动方向）：\n",
    "\\(output_i = \\sum_{j = 1}^{n} attention_{i,j} v_j\\)综合起来。\n",
    "自注意力机制的输出可以表示为：\n",
    "***\\(Attention(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\\)***\n",
    "\n",
    "### Q、K、V矩阵的生成\n",
    "#### 1、生成Q矩阵\n",
    "使用权重矩阵 \\(W^Q \\in \\mathbb{R}^{d_{model} \\times d_k}\\) 对输入序列 X 进行线性变换，从而得到 Query 矩阵 Q：\n",
    "\\(Q = XW^Q\\)\n",
    "这里的 \\(Q \\in \\mathbb{R}^{n \\times d_k}\\)，n 是序列的长度，\\(d_k\\) 是 Query 向量的维度。\n",
    "- ***形象解释***： 把 \\(W^Q\\) 想象成一个 “提问专家”。对于会议中的每个人 \\(x_i\\)，“提问专家” \\(W^Q\\) 会根据这个人的发言内容 \\(x_i\\) 生成一个 \\(d_k\\) 维的问题 \\(q_i\\)。所有这些问题组合起来就构成了 Query 矩阵 Q。这个问题代表着该元素在寻找相关信息时所关注的方向。\n",
    "#### 2、生成 Key 矩阵 \n",
    "K数学公式使用权重矩阵 \\(W^K \\in \\mathbb{R}^{d_{model} \\times d_k}\\) 对输入序列 X 进行线性变换，得到 Key 矩阵 K：\n",
    "\\(K = XW^K\\)\n",
    "这里的 \\(K \\in \\mathbb{R}^{n \\times d_k}\\)。\n",
    "- ***形象解释***：把 \\(W^K\\) 想象成一个 “信息标签专家”。对于会议中的每个人 \\(x_i\\)，“信息标签专家” \\(W^K\\) 会根据这个人的发言内容 \\(x_i\\) 生成一个 \\(d_k\\) 维的标签 \\(k_i\\)。所有这些标签组合起来就构成了 Key 矩阵 K。这个标签表示该元素能够提供的信息特征。\n",
    "#### 3、生成 Value 矩阵 \n",
    "V数学公式使用权重矩阵 \\(W^V \\in \\mathbb{R}^{d_{model} \\times d_v}\\) 对输入序列 X 进行线性变换，得到 Value 矩阵 V：\n",
    "\\(V = XW^V\\)\n",
    "这里的 \\(V \\in \\mathbb{R}^{n \\times d_v}\\)，\\(d_v\\) 是 Value 向量的维度，通常 \\(d_k = d_v\\)。\n",
    "- ***形象解释***：把 \\(W^V\\) 想象成一个 “信息提取专家”。对于会议中的每个人 \\(x_i\\)，“信息提取专家” \\(W^V\\) 会根据这个人的发言内容 \\(x_i\\) 提取出一个 \\(d_v\\) 维的信息片段 \\(v_i\\)。所有这些信息片段组合起来就构成了 Value 矩阵 V。这个信息片段是该元素实际包含的有价值信息。\n",
    "\n",
    "下图演示了这个过程：\n",
    "![3](Note_imags\\7.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 🕒 位置编码：给单词标上时间戳\n",
    "由于 Transformer 本身没有捕捉序列顺序信息的能力，不像RNN一个个接连输入，而是一起输入， 因此需要通过位置编码来为输入序列中的每个位置添加位置信息。\n",
    "Transformer给每个词添加位置信息：\n",
    "- 不同位置的词有不同的「位置向量」\n",
    "- 就像电影票的座位号，确保模型理解「猫追老鼠」和「老鼠追猫」的顺序差异\n",
    "位置编码通常使用正弦和余弦函数来生成：\\(PE_{(pos, 2i)} = \\sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})\\)\\(PE_{(pos, 2i + 1)} = \\cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})\\)其中 pos 是位置索引，i 是维度索引，\\(d_{model}\\) 是模型的维度。将位置编码 PE 与输入序列 X 相加，得到带有位置信息的输入：\\(X_{pos} = X + PE\\)\n",
    "\n",
    "**公式推导的核心**\n",
    "是让位置嵌入能体现 “相对位置”。举个例子：\n",
    "排队场景：假设队伍里有 A（位置 pos）和 B（位置 pos + k）。位置嵌入通过三角函数的线性组合（公式 3），让 B 的位置向量能被拆解成 A 的位置向量与 “间隔 k” 的向量组合。这就像在说：“B 的位置信息 = A 的位置信息 + 两人之间的间隔信息”。\n",
    "模型理解：这样一来，即使模型看到的是 B 的位置嵌入，也能通过这种数学关系，间接知道 B 和 A 的相对位置（前面还是后面，隔了多少人）。就像通过号码牌，不仅知道每个人的位置，还能算出谁离谁更近、谁在谁前面。\n",
    "![8](Note_imags\\8.png)\n",
    "\n",
    "\n",
    "### 🧠 多头注意力：多组专家同时分析\n",
    "#### 比喻：不同领域专家分别研究\n",
    "Transformer用**8组独立的注意力机制**（多头）：\n",
    "***原理理解***：\n",
    "- 每组关注不同的语义关系（如主谓、动宾、修饰关系）\n",
    "- 最后将各组结果合并，类似综合不同专家意见\n",
    "\n",
    "***计算步骤***\n",
    "设头的数量为 h，每个头的 Query、Key 和 Value 的维度为 \\(d_{k/h}\\) 和 \\(d_{v/h}\\)，满足 \\(d_k = h \\times d_{k/h}\\) 和 \\(d_v = h \\times d_{v/h}\\)。分割 Query、Key 和 Value：\n",
    "将 Q、K 和 V 分别分割成 h 个头：\n",
    "\\(Q_i = Q[:, :, i \\times d_{k/h}:(i + 1) \\times d_{k/h}]\\)\n",
    "\\(K_i = K[:, :, i \\times d_{k/h}:(i + 1) \\times d_{k/h}]\\)\n",
    "\\(V_i = V[:, :, i \\times d_{v/h}:(i + 1) \\times d_{v/h}]\\)\n",
    "其中 \\(i = 0, 1, \\cdots, h - 1\\)。计算每个头的注意力输出：\n",
    "对每个头分别计算自注意力：\n",
    "\\(head_i = Attention(Q_i, K_i, V_i)\\)拼接头的输出：\n",
    "将 h 个头的输出拼接起来：\n",
    "\\(concat = [head_0; head_1; \\cdots; head_{h - 1}]\\)线性变换得到最终输出：\n",
    "通过一个线性变换矩阵 \\(W^O \\in \\mathbb{R}^{h \\times d_{v/h} \\times d_{model}}\\) 对拼接后的结果进行变换：\n",
    "\\(MultiHead(Q, K, V) = concat W^O\\)\n",
    "\n",
    "### 残差\n",
    "#### 用“爬楼梯”比喻理解残差  \n",
    "想象你要从1楼到10楼，传统深度学习模型就像直接学习“怎么一口气从1楼到10楼”，这对模型来说太难了，越“学”越迷糊（深度增加时训练困难）。  \n",
    "\n",
    "而残差的思路是：**每次只学“上一层楼的高度差”**。比如：  \n",
    "- **输入\\( x \\)**：你现在在3楼（当前特征）。  \n",
    "- **\\( \\mathcal{F}(x) \\)**：这一层要学的“从3楼到4楼的高度差”（残差部分，即模型学习的变化量）。  \n",
    "- **输出\\( \\mathcal{F}(x) + x \\)**：把“高度差”和当前楼层（\\( x \\)）相加，就到4楼了。  \n",
    "![9](Note_imags/9.png)\n",
    "\n",
    "#### 残差的“三大贴心设计”  \n",
    "1. **降低学习难度**：  \n",
    "   传统模型要一口气学“1楼到10楼”，残差模型每次只学“上一层楼”，就像游戏里“每次只打弱一点的怪升级”，学习压力小很多。  \n",
    "2. **保留“原路”**：  \n",
    "   图中“\\( x \\) identity”就像给模型留了一条“原路”。哪怕“学高度差”没学好，至少还能通过“原路”保留之前的成果（比如保留底层图像的边缘特征），避免深度增加时“学丢了”。  \n",
    "3. **防止“学偏”**：  \n",
    "   深度网络容易因梯度问题“学偏”（梯度消失/爆炸），残差的“原路”让梯度能更顺畅地回传，就像给学习过程装了“稳定器”，模型训练更稳定，能轻松“盖高楼”（构建深层网络）。\n",
    "\n",
    "\n",
    "### Batch Normal  \n",
    "\n",
    "想象一群学生排队体检，但队伍混乱：  \n",
    "- 有的学生站得太密（数据分布集中），有的散在各处（数据分布分散），医生检查效率低（模型训练困难）。  \n",
    "\n",
    "**Batch Normal（批归一化）就像一位“队伍整理员”，做了两件事**：  \n",
    "1. **“排整齐”——标准化**：  \n",
    "   - 计算当前队伍（一个 Batch 的数据）的“平均位置”（均值）和“分散程度”（方差）。  \n",
    "   - 把每个学生的位置都调整到以“平均位置”为中心、固定“分散程度”的状态。就像让所有学生以队伍中点为基准，均匀分布，消除数据分布的混乱（解决内部协变量偏移问题）。  \n",
    "2. **“个性化装扮”——缩放平移**：  \n",
    "   - 给每个学生穿上不同的衣服（学习缩放因子\\(\\gamma\\)和平移因子\\(\\beta\\)）。  \n",
    "   - 这些“衣服”让学生在整齐的队伍中，还能保留自己的特点（恢复数据原本的表达能力，避免标准化抹除关键特征）。  \n",
    "\n",
    "#### Batch Normal 的好处  \n",
    "- **训练加速**：队伍整齐后，医生（模型）能快速检查每个学生（处理数据），迭代更快。  \n",
    "- **模型稳定**：无论来的是哪个班级（不同 Batch 的数据），都先整理成统一规范的队伍，模型不容易被“奇怪”的数据分布搞糊涂，训练更稳定。  \n",
    "- **缓解梯度问题**：标准化让数据分布更合理，梯度传播更顺畅，就像队伍通畅后，信息传递更高效，避免梯度消失或爆炸。\n",
    "\n",
    "### Layer Normal\n",
    "\n",
    "**核心思想**：  \n",
    "对每个样本的所有特征单独归一化，让每个样本的特征分布更合理。\n",
    "\n",
    "**比喻**：  \n",
    "假设有一群学生考试，每个学生考了语文、数学、英语三门课。  \n",
    "- **传统Batch Normalization**：老师计算全班每门课的平均分和标准差，把每个学生的成绩按班级标准调整（比如语文全班平均分80，标准差5，学生A的90分变成2.0）。  \n",
    "- **Layer Normalization**：老师让每个学生自己计算自己三门课的平均分和标准差，调整自己的成绩（比如学生A语文90、数学70、英语80 → 平均分80，标准差≈8.16 → 各科成绩变成1.22、-1.22、0）。\n",
    "\n",
    "**优势**：  \n",
    "1. **不依赖班级大小**：即使只有1个学生，也能独立调整。  \n",
    "2. **适应科目变化**：新增课程时，学生自己重新计算即可。  \n",
    "3. **保留个体差异**：每个学生根据自己的情况调整，避免“一刀切”。\n",
    "\n",
    "**典型应用**：  \n",
    "- **Transformer模型**：处理句子时，每个词向量的所有维度单独归一化，确保不同位置的词在同一尺度上比较。  \n",
    "- **自然语言处理**：适合长短不一的句子（如机器翻译、文本生成）。\n",
    "\n",
    "**一句话总结**：  \n",
    "Layer Normalization是每个样本“自己调整各科成绩”，让模型更稳定地处理不同长度的序列数据。\n",
    "\n",
    "### 解码器\n",
    "由多个相同的解码器层堆叠而成，每个解码器层包含三个子层：掩码多头注意力子层、编码器 - 解码器多头注意力子层和前馈神经网络子层。1. 掩码多头注意力子层为了防止解码器在生成序列时提前看到未来的信息，需要在多头注意力机制中使用掩码。掩码多头注意力的输出为 \\(MaskedMHA(Y)\\)，然后通过残差连接和层归一化：\n",
    "\\(Y_1 = \\text{LayerNorm}(Y + MaskedMHA(Y))\\)\n",
    "其中 Y 是解码器的输入。2. 编码器 - 解码器多头注意力子层解码器使用编码器的输出 \\(X_2\\) 作为 Key 和 Value，解码器自身的输出 \\(Y_1\\) 作为 Query，计算多头注意力 \\(MHA(Y_1, X_2, X_2)\\)，然后通过残差连接和层归一化：\n",
    "\\(Y_2 = \\text{LayerNorm}(Y_1 + MHA(Y_1, X_2, X_2))\\)3. 前馈神经网络子层与编码器中的前馈神经网络子层类似，计算 \\(FFN(Y_2)\\) 并通过残差连接和层归一化：\n",
    "\\(Y_3 = \\text{LayerNorm}(Y_2 + FFN(Y_2))\\)最终输出解码器的最终输出通过一个线性层和 Softmax 函数得到每个位置的概率分布，用于预测目标序列中的每个元素。\n",
    "\n",
    "### 🚀 优势：并行处理 vs 传统模型\n",
    "#### 对比：流水线 vs 接力赛\n",
    "- **LSTM**：必须按顺序处理每个词（像接力赛）\n",
    "- **Transformer**：所有词同时处理（像流水线）\n",
    "- 因此Transformer速度更快，且能捕捉长距离依赖（如相隔很远的两个词的关系）\n",
    "\n",
    "\n",
    "### 🌰 总结：翻译「猫追老鼠」的过程\n",
    "1. **编码器**：\n",
    "   - 每个词通过自注意力理解上下文关系\n",
    "   - 输出包含全局信息的「理解向量」\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 🌈 为什么Transformer如此强大？\n",
    "- **并行计算**：处理速度比LSTM快10倍以上\n",
    "- **长距离依赖**：能捕捉相隔千词的语义关系\n",
    "- **迁移能力**：预训练后微调即可用于各种NLP任务（如问答、文本生成）\n",
    "\n",
    "如果把语言理解比作拼图游戏，Transformer就像能同时看到所有拼图碎片，并快速找到最佳拼接方式的超级玩家。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![示例图片](Note_imags\\6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 编码器和解码器\n",
    "![示例图片](Note_imags\\5.png)\n",
    "#### 有多个编码器和解码器，不过每个编码器解码器在更新参数时，不是一样的，不像RNN是共享的。\n",
    "![示例图片](Note_imags\\4.png)\n",
    "#### 原论文中transformer的结构图\n",
    "- 左边是编码器，右边是解码器\n",
    "![示例图片](Note_imags\\3.png)\n",
    "#### 对每个字所占的512维向量分别进行编码，偶数用sin，奇数用cos\n",
    "![示例图片](Note_imags\\2.png)\n",
    "#### 将位置编码后的向量与原向量相加得到新的\n",
    "![示例图片](Note_imags\\1.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformer并不像RNN那样，一个一个词的依次处理，而是一起，没有时间，需要位置编码"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
